{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b1bfbc-02ee-4fa9-8357-92d62b611850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6584b998-49e0-42eb-a102-b9a00e5e8da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             utterance  label\n",
      "0            \\talright      0\n",
      "1  \\tyou have fun okay      0\n",
      "2               \\tokay      1\n",
      "3            \\talright      0\n",
      "4   \\tdid you see this      0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('fil_data.csv')\n",
    "\n",
    "print(df.head())\n",
    "df.dropna(subset=['utterance', 'label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c03503-bce0-45f4-a20e-cd8b2bbb7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_sentence_length(text):\n",
    "    sentences = text.split('.')  # Split by period (you can refine this for more accurate sentence splitting)\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence.strip() != '']  # Count words per sentence\n",
    "    if len(sentence_lengths) == 0:\n",
    "        return 0\n",
    "    return np.mean(sentence_lengths)  # Return the average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1c119e-3f5a-45b6-a12a-1ab8fe7afebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the linguistic feature for all utterances\n",
    "df['avg_sentence_length'] = df['utterance'].apply(calculate_avg_sentence_length)\n",
    "\n",
    "# Normalize the linguistic feature\n",
    "scaler = MinMaxScaler()\n",
    "df['avg_sentence_length'] = scaler.fit_transform(df[['avg_sentence_length']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d75543-d2c5-4e1a-897f-2eaaecca81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 2000  # Limit vocabulary size for faster training\n",
    "embedding_dim = 100\n",
    "max_length = 50  # Truncate or pad sequences to this length\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057b57f2-e48f-45f9-8a1b-ab2b8e380d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the text and labels\n",
    "texts = df['utterance'].values\n",
    "linguistic_features = df['avg_sentence_length'].values  # Now using avg sentence length as the feature\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141043c8-a557-4d2f-a1ea-a16cc3ff6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0573b2e1-dc30-4074-85bc-0c8862a49c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine padded sequences and linguistic features\n",
    "X_combined = np.hstack((padded_sequences, linguistic_features.reshape(-1, 1)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb0bada-b96c-48bd-81fc-305590e2d123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SRI SHIKA.L\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Utterance input branch\n",
    "input_1 = Input(shape=(max_length,), name=\"utterance_input\")\n",
    "embedding = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_1)\n",
    "\n",
    "#Bidirectional LSTM layers\n",
    "bilstm_out = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
    "bilstm_out = Dropout(0.2)(bilstm_out)\n",
    "bilstm_out = Bidirectional(LSTM(64, return_sequences=True))(bilstm_out)  # Second Bidirectional LSTM layer\n",
    "bilstm_out = Dropout(0.2)(bilstm_out)\n",
    "bilstm_out = Bidirectional(LSTM(32))(bilstm_out)  # Final Bidirectional LSTM layer#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b83710f1-e7c4-43f8-a1f4-c85a07c637e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linguistic feature input branch\n",
    "input_2 = Input(shape=(1,), name=\"linguistic_feature_input\")\n",
    "dense_feat = Dense(16, activation='relu')(input_2)\n",
    "\n",
    "#Combining both branches\n",
    "combined = concatenate([bilstm_out, dense_feat])\n",
    "dense_out = Dense(64, activation='relu')(combined)\n",
    "dense_out = Dropout(0.2)(dense_out)\n",
    "final_output = Dense(1, activation='sigmoid')(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0324957b-1c62-4b87-834d-4bcf0bced335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1628/1628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 187ms/step - accuracy: 0.7699 - loss: 0.4805 - val_accuracy: 0.8105 - val_loss: 0.4244\n",
      "Epoch 2/5\n",
      "\u001b[1m1628/1628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 206ms/step - accuracy: 0.8199 - loss: 0.4040 - val_accuracy: 0.8136 - val_loss: 0.4115\n",
      "Epoch 3/5\n",
      "\u001b[1m1628/1628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 166ms/step - accuracy: 0.8217 - loss: 0.3871 - val_accuracy: 0.8162 - val_loss: 0.4154\n",
      "Epoch 4/5\n",
      "\u001b[1m1628/1628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 148ms/step - accuracy: 0.8320 - loss: 0.3696 - val_accuracy: 0.8165 - val_loss: 0.4123\n",
      "Epoch 5/5\n",
      "\u001b[1m1628/1628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 141ms/step - accuracy: 0.8390 - loss: 0.3554 - val_accuracy: 0.8225 - val_loss: 0.4103\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 46ms/step\n",
      "Accuracy: 0.8225499231950845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86     10430\n",
      "           1       0.74      0.78      0.76      5845\n",
      "\n",
      "    accuracy                           0.82     16275\n",
      "   macro avg       0.81      0.81      0.81     16275\n",
      "weighted avg       0.82      0.82      0.82     16275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input_1, input_2], outputs=final_output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 40\n",
    "history = model.fit(\n",
    "    [X_train[:, :-1], X_train[:, -1]], y_train,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=([X_test[:, :-1], X_test[:, -1]], y_test)\n",
    ")\n",
    "\n",
    "y_pred = (model.predict([X_test[:, :-1], X_test[:, -1]]) > 0.5).astype(\"int32\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d42eaf-a98d-4dea-8f88-b604e4bdb19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
